{"cells":[{"source":"![servicedesk](servicedesk.png)\n\nCleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.","metadata":{"executionCancelledAt":null,"executionTime":165,"lastExecutedAt":1707667023665,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."},"id":"e5870ae0-6165-459e-9c40-0f282883be7b","cell_type":"markdown"},{"source":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1749846918107,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c"},"id":"2fa90b61-0244-4236-aa93-e33a7a088eec","cell_type":"code","execution_count":133,"outputs":[]},{"source":"nltk.download('punkt')","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1749846918156,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nltk.download('punkt')","outputsMetadata":{"0":{"height":80,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c"},"id":"37a51a81-1301-4a80-b8c6-716faaff4c5c","cell_type":"code","execution_count":134,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":134}]},{"source":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')","metadata":{"executionCancelledAt":null,"executionTime":118,"lastExecutedAt":1749846918274,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')"},"id":"e1b12eaf-e55c-422c-94a2-b0197c465a1b","cell_type":"code","execution_count":135,"outputs":[]},{"source":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)","metadata":{"executionCancelledAt":null,"executionTime":227,"lastExecutedAt":1749846918503,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)"},"id":"d630badb-23dd-4368-9a96-e2b478ad5cff","cell_type":"code","execution_count":136,"outputs":[]},{"source":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749846918551,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c"},"id":"f2654836-631f-415e-9922-5ab3bafaaafa","cell_type":"code","execution_count":137,"outputs":[]},{"source":"train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=400, shuffle=False)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1749846918599,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_loader = DataLoader(train_data, batch_size=400, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=400, shuffle=False)"},"id":"d2b3c50c-66b1-40ed-a17c-038e7addc7ec","cell_type":"code","execution_count":138,"outputs":[]},{"source":"len(set(labels))","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1749846918647,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"len(set(labels))"},"cell_type":"code","id":"75d344d2-ab50-4ef0-801a-c88626022f19","outputs":[{"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{},"execution_count":139}],"execution_count":139},{"source":"class TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1, stride=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embed = self.embedding(text).permute(0,2,1)\n        conved = F.relu(self.conv(embed))\n        conved = conved.mean(dim=2)\n        return self.fc(conved)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1749846918700,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, padding=1, stride=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embed = self.embedding(text).permute(0,2,1)\n        conved = F.relu(self.conv(embed))\n        conved = conved.mean(dim=2)\n        return self.fc(conved)"},"cell_type":"code","id":"5bcb4489-6f03-4fde-b08f-86b0a58badc0","outputs":[],"execution_count":140},{"source":"model = TicketClassifier(len(word2idx) + 1, 64, len(set(labels)))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1749846918752,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model = TicketClassifier(len(word2idx) + 1, 64, len(set(labels)))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)"},"cell_type":"code","id":"7414f02d-0cee-49ce-9011-70c96f4d0bb0","outputs":[],"execution_count":141},{"source":"epochs = 3\n\nmodel.train()\nfor epoch in range(epochs):\n    running_loss, num_processed = 0, 0\n    for text, label in train_loader:\n        model.zero_grad()\n        output = model(text)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(text)\n    print(f\"Epoch: {epoch+1}, Loss: {running_loss/num_processed}\")","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"dba72832-256a-4a1a-831d-6c6f8a3d6110","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, Loss: 0.003693235069513321\nEpoch: 2, Loss: 0.0016779607981443406\nEpoch: 3, Loss: 0.0007226967178285122\n"}],"execution_count":142},{"source":"acc = Accuracy(task=\"multiclass\", num_classes=5)\npre = Precision(task=\"multiclass\", num_classes=5, average=None)\nrec = Recall(task=\"multiclass\", num_classes=5, average=None)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1749846921571,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"acc = Accuracy(task=\"multiclass\", num_classes=5)\npre = Precision(task=\"multiclass\", num_classes=5, average=None)\nrec = Recall(task=\"multiclass\", num_classes=5, average=None)"},"cell_type":"code","id":"d67f9455-bb77-414f-85ce-c3c095542191","outputs":[],"execution_count":143},{"source":"model.eval()\npredicted= []\n\nfor _, (text, label) in enumerate(test_loader):\n    output = model(text)\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    acc(cat, label)\n    pre(cat, label)\n    rec(cat, label)","metadata":{"executionCancelledAt":null,"executionTime":61,"lastExecutedAt":1749846921632,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model.eval()\npredicted= []\n\nfor _, (text, label) in enumerate(test_loader):\n    output = model(text)\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    acc(cat, label)\n    pre(cat, label)\n    rec(cat, label)"},"cell_type":"code","id":"a6663c48-d9f9-4257-ae95-8e567f94b030","outputs":[],"execution_count":144},{"source":"accuracy = acc.compute().item()\nprecision = pre.compute().tolist()\nrecall = rec.compute().tolist()","metadata":{"executionCancelledAt":null,"executionTime":83,"lastExecutedAt":1749846921715,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"accuracy = acc.compute().item()\nprecision = pre.compute().tolist()\nrecall = rec.compute().tolist()"},"cell_type":"code","id":"4a23be8a-a565-4100-a796-cbc869235eb9","outputs":[],"execution_count":145},{"source":"print(\"Accuracy(per class):\", accuracy, \"\\nPrecision(per class):\", precision, \"\\nRecall(per class):\", recall)","metadata":{"executionCancelledAt":null,"executionTime":44,"lastExecutedAt":1749846921759,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"Accuracy(per class):\", accuracy, \"\\nPrecision(per class):\", precision, \"\\nRecall(per class):\", recall)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"cell_type":"code","id":"d7236187-2c69-4a49-8119-5c33f1230004","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy(per class): 0.7910000085830688 \nPrecision(per class): [0.65887850522995, 0.8157894611358643, 0.7599999904632568, 0.8787878751754761, 0.8721461296081543] \nRecall(per class): [0.734375, 0.6526315808296204, 0.8796296119689941, 0.7552083134651184, 0.9095237851142883]\n"}],"execution_count":146},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1749846921807,"lastExecutedByKernel":"f1b2a69c-0860-4a88-b993-13bf0ad9224c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"24a195a6-e652-4d29-bbbc-8beb78708454","outputs":[],"execution_count":146}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}